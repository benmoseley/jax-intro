{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aac18f8",
   "metadata": {},
   "source": [
    "# What is JAX?\n",
    "Ben Moseley\n",
    "9 December 2022\n",
    "\n",
    "This material has mostly been copied from the official JAX documentation:\n",
    "- https://jax.readthedocs.io/en/latest/notebooks/quickstart.html\n",
    "\n",
    "And these blog posts:\n",
    "- https://www.assemblyai.com/blog/why-you-should-or-shouldnt-be-using-jax-in-2022/\n",
    "- https://kidger.site/thoughts/jax-vs-julia/\n",
    "\n",
    "\n",
    "<img src=\"JAX-overview.svg\" width=40%>\n",
    "\n",
    "> JAX and Julia are unquestionably the current state-of-the-art frameworks for autodifferentiation, scientific computing, and ML computing\n",
    "\n",
    "Compared to PyTorch:\n",
    "- JAX is *much faster* than pytorch for scientific computing (less interpreter overhead/ better jit) - particularly for e.g. differential equation solvers\n",
    "- JAX is *more feature-complete* for scientific computing - e.g. more advanced autodifferentiation\n",
    "\n",
    "## Programming paradigm\n",
    "\n",
    "\n",
    "### Compilation\n",
    "\n",
    "Julia and JAX are both based on JIT compilation - compiled and optimised down to efficient machine code the first time it is run.\n",
    "\n",
    "The first run of the code (function) is slower, whilst subsequent runs are fast.\n",
    "\n",
    "Compared to Julia:\n",
    "- JAX can have slowish compilation speeds for the python interpreter to chug through, compared to Julia\n",
    "- JAX is a python package (DSL), whilst Julia is a programming language. This means we have to refer to the `jax` package rather than using native synatax such as `for` loops, for some use cases\n",
    "- JAX appears more reliable (in terms of documentation, gradient bugs/ code reliability, professionalism), and array indexing is much nicer; it mirrors numpy\n",
    "\n",
    "JAX relies on JIT XLA to make this compiled code fast (*whole program compiler* designed for linear algebra, and not just single operations): XLA (developed by Google) is the DNA and foundation of JAX, and includes many optimisations including *kernel fusion*.\n",
    "\n",
    "Note this can place limitations on the types of code (functions) that you can jit (see below).\n",
    "\n",
    "Since all JAX operations are implemented in terms of operations in XLA, JAX has a *unified language* for computation that allows it to run seamlessly across CPU, TPU, and GPU.\n",
    "\n",
    "Compared to PyTorch:\n",
    "- JAX uses static graph creation and compilation, whilst PyTorch is dynamic\n",
    "\n",
    "### Composible function transforms\n",
    "\n",
    "JAX (and Julia) uses *homoiconicity* - the property of being able to represent *code as data*, or more precisely, the program's internal representation can be inferred just by reading the program itself.\n",
    "\n",
    "> This allows you to *write code that modifies other code*. E.g. backpropagation: reriting your code to be evaluated in reverse order. Aka metaprogramming (other programs as input data -> a program can read, generate, analyse and transform other programs). This is known as a *program transformation* (`jaxprs` deals with this) including: \n",
    "> - JIT compilation\n",
    "> - Autodifferentiation\n",
    "> - Automatic parallelisation\n",
    "> - Automatic vectorisation\n",
    "\n",
    "Jax provides four main transforms (functions to functions) which can be composed *arbitrarily*:\n",
    "1. `grad()` for evaluating the gradient of a function, as well as `jacfwd()` and `jacrev()` and jvp/vjp. Forward/reverse, composed arbitrarily to any order\n",
    "2. `vmap()` for automatic vectorisation (*beyond simple vectorisation*, to arbitrary Python functions)\n",
    "3. `pmap()` for easy parallelisation (across cores/GPUs)\n",
    "4. `jit()` for compilation\n",
    "\n",
    "> JAX USP: High performance numerical computing library which combines:\n",
    "> - JIT XLA compilation\n",
    "> - *composable transformations* of Python+NumPy programs (/functions) (differentiate, vectorize, JIT to GPU/TPU, and more)\n",
    "\n",
    "Note deep learning is just a small subset of what JAX can do (see figure)!\n",
    "\n",
    "Behind the scenes of JAX: https://jax.readthedocs.io/en/latest/autodidax.html\n",
    "\n",
    "Compared to PyTorch:\n",
    "- Fundamentally JAX is a stack of interpreters, that go through and progressively re-write your program -- e.g. mapping over batch dimensions, take gradients etc. -- before offloading all the actual computation to XLA\n",
    "- In torch, the graph is built in the forward pass, whilst gradients are calculated in the backwards pass, for a certain point in the function. In JAX, a new *function* is created which directly computes the gradient of any input function\n",
    "- This appears more flexible and combined with JIT XLA e.g. allows more efficient computation of gradient-per-sample and Hessians and other more complex autodiff use cases\n",
    "- Both frameworks can differentiate through complex control flows\n",
    "\n",
    "## CPU / GPU support\n",
    "\n",
    "JAX appears to fully support GPUs when using the composable transforms above. \n",
    "\n",
    "JAX provides a near-identical numpy version, which works very easily on GPUs and TPUs. This alone can justify the use of JAX!\n",
    "\n",
    "JAX for TPU is absolutely seamless.\n",
    "\n",
    "JAX is not optimised for CPU computing (\"accelerator first\") - so numpy may be faster for some cases.\n",
    "\n",
    "## Development\n",
    "\n",
    "JAX is being actively and professionally developed, although still officially experimental. Hit the scene in 2018, used by many Google/DeepMind papers now. Not Windows compatible.\n",
    "\n",
    "\n",
    "## What's the catch?\n",
    "\n",
    "### Functional programming\n",
    "\n",
    "The catch of composible program transforms and JIT XLA is that this restricts JAX to work only for *functionally pure* programs: we must be able to write in terms of *pure functions* (and not objects or other more complex control flows). \n",
    "\n",
    "All the input data is passed through the function parameters, all the results are output through the function results. A pure function will always return the same result if invoked with the same inputs.\n",
    "\n",
    "Another way of saying this is that the function must have *referential transparency* - a pure function can be replaced with the result of its evaluation at any time, and the program cannot tell the difference. \n",
    "\n",
    "The function:\n",
    "- cannot have control flow statements that depend on values of variables\n",
    "- cannot access variables outside of its scope\n",
    "- cannot have an I/O stream (so no printing, or accessing the time)\n",
    "- cannot have a mutable function as argument\n",
    "\n",
    "Other limitations:\n",
    "- JAX *cannot modify arrays in-place*.\n",
    "- JAX requires *explicit RNG handling*.\n",
    "- errors are *not* raised for out-of-bounds indexing. (!) Think of this as undefined behaviour.\n",
    "- It is not recommended to use iterators in any JAX function you want to `jit`.\n",
    "\n",
    "\n",
    "Otherwise, there may be some !! untracked side effects !!\n",
    "\n",
    "> Make sure you are dilligent and understand functional programming / JAX pitfalls. \n",
    "\n",
    "Jax is lower-level than PyTorch.\n",
    "\n",
    "## SciML\n",
    "\n",
    "JAX is brilliant for large-scale, complex, hybrid modeling, and soley for hardware acceleration too!\n",
    "\n",
    "## Deep learning\n",
    "\n",
    "JAX is roughly equivalent to just the `torch` namespace; you need to choose from various external libraries for building neural networks (equivalent to the `torch.nn` namespace). Check out `Equinox` - simple/transparent/general/designed with DEs in mind/ flexible function parameterisation. `Flax` (Google Brain) and `Haiku` (DeepMind) are most popular. `Optax` is good for optimisers.\n",
    "\n",
    "JAX is excellent for non-standard modelling, and training on TPUs! For standard training, PyTorch is probably better for now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "299e45db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "from jax import grad, jit, vmap, pmap,  jacfwd, jacrev\n",
    "from jax import random\n",
    "from jax import device_put, make_jaxpr\n",
    "from jax import tree_util\n",
    "from jax.scipy.special import logsumexp\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038dfc8f",
   "metadata": {},
   "source": [
    "# Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecca87b8",
   "metadata": {},
   "source": [
    "## Simple arrays\n",
    "\n",
    "- JAX provides a NumPy-inspired interface `jax.numpy` for convenience.\n",
    "- Through duck-typing, JAX arrays can often be used as drop-in replacements of NumPy arrays.\n",
    "- Unlike NumPy arrays, JAX arrays are always immutable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e45889f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.3721109   0.26423115 -0.18252768 -0.7368197  -0.44030377 -0.1521442\n",
      " -0.67135346 -0.5908641   0.73168886  0.5673026 ] (10,) <class 'jaxlib.xla_extension.DeviceArray'>\n"
     ]
    }
   ],
   "source": [
    "# generate random data\n",
    "key = random.PRNGKey(0)\n",
    "x = random.normal(key, (10,))# JAX requires explicit RNG handling (see below)\n",
    "print(x, x.shape, type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54ecd173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[129.95944   ,   7.6958685 ,   8.772064  , ...,\n",
       "                6.3571653 ,   2.3293855 ,  -8.627743  ],\n",
       "             [  7.6958685 , 123.0987    ,   1.9893165 , ...,\n",
       "                0.45828867,  -9.394791  , -15.0048275 ],\n",
       "             [  8.772064  ,   1.9893165 ,  87.803734  , ...,\n",
       "              -21.56633   ,  16.132122  ,  -5.47758   ],\n",
       "             ...,\n",
       "             [  6.3571653 ,   0.45828724, -21.566336  , ...,\n",
       "              126.38751   ,  -2.683978  ,   2.3173656 ],\n",
       "             [  2.3293874 ,  -9.394792  ,  16.13212   , ...,\n",
       "               -2.683978  , 120.720474  ,   1.6255394 ],\n",
       "             [ -8.627744  , -15.004827  ,  -5.4775796 , ...,\n",
       "                2.3173656 ,   1.6255394 ,  87.94435   ]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix multiply\n",
    "x = random.normal(key, (100, 120), dtype=jnp.float32)\n",
    "x@x.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c0d667",
   "metadata": {},
   "source": [
    "The big difference is that JAX arrays are *immutable*!\n",
    "\n",
    "This is because allowing mutation of variables in-place makes program analysis and transformation difficult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c5cc29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<class 'jaxlib.xla_extension.DeviceArray'>' object does not support item assignment. JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html\n"
     ]
    }
   ],
   "source": [
    "x = jnp.arange(10)\n",
    "try:\n",
    "    x[0] = 10\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27753b72",
   "metadata": {},
   "source": [
    "For updating individual elements, JAX provides an indexed update syntax that returns an updated out-of-place *copy*\n",
    "(a functional array update).\n",
    "\n",
    "However, inside jit-compiled code, if the input value `x` of `x.at[idx].set(y)` is not reused, the compiler will optimize the array update to occur in-place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a504625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10  1  2  3  4  5  6  7  8  9]\n",
      "[17  1  9  3 11  5 13  7 15  9]\n"
     ]
    }
   ],
   "source": [
    "x = x.at[0].set(10)\n",
    "print(x)\n",
    "x = x.at[::2].add(7)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13c12f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad2bcfe2",
   "metadata": {},
   "source": [
    "### Random numbers\n",
    "\n",
    "JAX implements an explicit PRNG where randomness is handled by *explicitly* passing and iterating a PRNG state.\n",
    "\n",
    "JAX uses a modern Threefry counter-based PRNG that’s *splittable*. That is, its design allows us to *fork* the PRNG state into new PRNGs for use with parallel stochastic generation.\n",
    "\n",
    "This gives much better control over PRNG for complex multi threaded/process/device programs.\n",
    "\n",
    "Note: JAX’s random functions produce pseudorandom numbers from the PRNG state, but *do not* change the state!\n",
    "\n",
    "Instead, we split the PRNG to get usable subkeys every time we need a new pseudorandom number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e50fdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.20584226]\n",
      "[-0.20584226]\n",
      "[-1.2515389]\n",
      "[1.4544677]\n",
      "[-0.49327764]\n",
      "[-0.39947626]\n"
     ]
    }
   ],
   "source": [
    "key = random.PRNGKey(0)# key represents state and is passed as first argument\n",
    "print(random.normal(key, shape=(1,)))# does not change the state (!)\n",
    "print(random.normal(key, shape=(1,)))\n",
    "\n",
    "key, subkey = random.split(key)# propagates the key, and produces a subkey for use\n",
    "print(random.normal(subkey, shape=(1,)))\n",
    "\n",
    "key, *subkeys = random.split(key, 4)# generate multiple subkeys at once\n",
    "for subkey in subkeys:\n",
    "    print(random.normal(subkey, shape=(1,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc478580",
   "metadata": {},
   "source": [
    "### `jax.numpy` vs `jax.lax`\n",
    "\n",
    "- jax.numpy is a high-level wrapper that provides a familiar interface.\n",
    "- jax.lax is a lower-level API that is stricter and often more powerful.\n",
    "\n",
    "If you look at the source of `jax.numpy`, you’ll see that all the operations are eventually expressed in terms of functions defined in `jax.lax`.\n",
    "\n",
    "For example, while `jax.numpy` will implicitly promote arguments to allow operations between mixed data types, `jax.lax` will not.\n",
    "\n",
    "At their heart, all `jax.lax` operations are Python wrappers for operations in XLA.\n",
    "\n",
    "Every JAX operation is eventually expressed in terms of these fundamental XLA operations, which is what enables JIT compilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7edd96f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(2., dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lax.add(jnp.float32(1), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310d1e48",
   "metadata": {},
   "source": [
    "## Putting onto device\n",
    "\n",
    "In JAX, the computation follows data placement. JAX arrays have two placement properties: 1) the device where the data resides; and 2) whether it is committed to the device or not (the data is sometimes referred to as being sticky to the device).\n",
    "\n",
    "By default, JAX arrays are placed uncommitted on the default device (`jax.devices()[0]`), which is the first GPU or TPU by default. If no GPU or TPU is present, `jax.devices()[0]` is the CPU. The default device can temporarily overridden with the `jax.default_device()` context manager, or set for the whole process by setting the environment variable `JAX_PLATFORMS` or the `absl flag --jax_platforms` to `“cpu”`, `“gpu”`, or `“tpu”`\n",
    "\n",
    "Computations involving uncommitted data are performed on the default device and the results are uncommitted on the default device.\n",
    "\n",
    "Data can also be placed explicitly on a device using `jax.device_put()` with a device parameter, in which case the data becomes committed to the device.\n",
    "\n",
    "Computations involving some committed inputs will happen on the committed device and the result will be committed on the same device. Invoking an operation on arguments that are committed to more than one device will raise an error.\n",
    "\n",
    "`jax.device_put(jnp.zeros(...)`, `jax.devices()[1])` or similar will actually create the array of zeros on `jax.devices()[1]`, instead of creating the array on the default device then moving it. This is thanks to some laziness in array creation, which holds for all the constant creation operations (ones, full, eye, etc).\n",
    "\n",
    "Thus, JAX runs transparently on the GPU or TPU (falling back to CPU if you don’t have one). \n",
    "\n",
    "Use `device_put` to put *and keep* something on a device. It only copies values back to the CPU when they’re needed for printing, plotting, saving to disk, branching, etc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19ddd22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CpuDevice(id=0)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CpuDevice(id=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(jax.devices())\n",
    "\n",
    "x = device_put(x)\n",
    "x.device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef7c684",
   "metadata": {},
   "source": [
    "## Asynchronous dispatch\n",
    "\n",
    "JAX uses asynchronous dispatch to hide Python overhead. When an operation such as `jnp.dot(x, x)` is executed, JAX does not wait for the operation to complete before returning control to the Python program.\n",
    "\n",
    "Only if we actually inspect the value of the array from the host, for example by printing it or by converting it into a plain old `numpy.ndarray` will JAX force the Python code to wait for the computation to complete.\n",
    "\n",
    "One can also call the `.block_until_ready()` method on a `DeviceArray` to wait for the computation to complete.\n",
    "\n",
    "Asynchronous dispatch is useful since it allows Python code to “run ahead” of an accelerator device, keeping Python code out of the critical path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0da8265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(1050, dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x@x.T).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55a86ff",
   "metadata": {},
   "source": [
    "## How JAX transforms work\n",
    "\n",
    "Conceptually, one can think of JAX transformations as first trace-specializing the Python function to be transformed into a small and well-behaved intermediate form that is then interpreted with transformation-specific interpretation rules. One of the reasons JAX can pack so much power into such a small software package is that it starts with a familiar and flexible programming interface (Python with NumPy) and it uses the actual Python interpreter to do most of the heavy lifting to distill the essence of the computation into a simple statically-typed expression language with limited higher-order features. That language is the `jaxpr` language.\n",
    "\n",
    "It is important to point out that not all JAX transformations literally materialize a `jaxpr` as described above; some, e.g., differentiation or batching, will apply transformations incrementally during tracing. \n",
    "\n",
    "\n",
    "In the previous section, we discussed that JAX allows us to transform Python functions. This is done by first converting the Python function into a simple intermediate language called jaxpr. The transformations then work on the `jaxpr` representation.\n",
    "\n",
    "We can show a representation of the `jaxpr` of a function by using `jax.make_jaxpr`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd4808b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; a\u001b[35m:f32[]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
      "    \u001b[39m\u001b[22m\u001b[22mb\u001b[35m:f32[]\u001b[39m = log a\n",
      "    c\u001b[35m:f32[]\u001b[39m = log 2.0\n",
      "    d\u001b[35m:f32[]\u001b[39m = div b c\n",
      "  \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(d,) }\n"
     ]
    }
   ],
   "source": [
    "global_list = []\n",
    "\n",
    "def log2(x):\n",
    "    global_list.append(x)# jax functions shouldn't modify external variables - tracing does not record side effects!!\n",
    "    ln_x = jnp.log(x)\n",
    "    ln_2 = jnp.log(2.0)\n",
    "    return ln_x / ln_2\n",
    "\n",
    "print(jax.make_jaxpr(log2)(3.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d56aad4",
   "metadata": {},
   "source": [
    "Importantly, note how the jaxpr *does not capture the side-effect* of the function: there is nothing in it corresponding to global_list.append(x). This is a feature, not a bug: \n",
    "\n",
    "> JAX is designed to understand side-effect-free (a.k.a. functionally pure) code.\n",
    "\n",
    "> In order to use jaxpr and tracing, JAX code must be *functionally pure*.\n",
    "\n",
    "This means that functions must not have *side-effects*:\n",
    "\n",
    "A side-effect is any effect of a function that doesn’t appear in its output.\n",
    "\n",
    "Of course, impure functions can still be written and even run, but JAX gives no guarantees about their behaviour once converted to jaxpr. However, as a rule of thumb, you can expect (but shouldn’t rely on) the side-effects of a JAX-transformed function to run once (during the first call), and never again. This is because of the way that JAX generates jaxpr, using a process called ‘tracing’.\n",
    "\n",
    "Any such side-effects will only be executed once, when the python version of the function is run during compilation. These side-effects will not be executed by the compiled function on subsequent runs.\n",
    "\n",
    "When tracing, JAX wraps each argument by a tracer object. These tracers then record all JAX operations performed on them during the function call (which happens in regular Python). Then, JAX uses the tracer records to reconstruct the entire function. The output of that reconstruction is the jaxpr. Since the tracers do not record the Python side-effects, they do not appear in the jaxpr. However, the side-effects still happen during the trace itself.\n",
    "\n",
    "Note: the Python `print()` function is not pure: the text output is a side-effect of the function. Therefore, any `print()` calls will only happen during tracing, and will not appear in the jaxpr.\n",
    "\n",
    "Of course, it’s possible to mix side-effectful Python code and functionally pure JAX code, and we will touch on this more later. As you get more familiar with JAX, you will learn how and when this can work. As a rule of thumb, however, any functions intended to be transformed by JAX should avoid side-effects, and the JAX primitives themselves will try to help you do that.\n",
    "\n",
    "A key thing to understand is that jaxpr captures the function as executed on the parameters given to it. For example, if we have a conditional, jaxpr will only know about the branch we take:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebeabb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; a\u001b[35m:i32[3]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\u001b[39m\u001b[22m\u001b[22m  \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(a,) }\n"
     ]
    }
   ],
   "source": [
    "def log2_if_rank_2(x):\n",
    "    if x.ndim == 2:\n",
    "        ln_x = jnp.log(x)\n",
    "        ln_2 = jnp.log(2.0)\n",
    "        return ln_x / ln_2\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "print(jax.make_jaxpr(log2_if_rank_2)(jax.numpy.array([1, 2, 3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "964ab43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; a\u001b[35m:i32[]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\u001b[39m\u001b[22m\u001b[22m b\u001b[35m:i32[]\u001b[39m = mul 2 a \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(b,) }\n",
      "3\n",
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "w = 2\n",
    "\n",
    "def m(x):\n",
    "    return w*x# be very careful with referencing external variables\n",
    "jit_m = jit(m)\n",
    "\n",
    "print(jax.make_jaxpr(m)(1))\n",
    "\n",
    "for i in range(3):\n",
    "    w += 1\n",
    "    print(jit_m(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b98c61",
   "metadata": {},
   "source": [
    "## Using `jit()`\n",
    "\n",
    "By default, JAX executes operations to the GPU/CPU *one at a time*, in sequence. If we have a sequence of operations, we can use the `@jit` decorator to *compile and optimise multiple operations together* using XLA (kernel fusion).\n",
    "\n",
    "The fact that all JAX operations are expressed in terms of XLA allows JAX to use the XLA compiler to execute blocks of code very efficiently.\n",
    "\n",
    "This is super fast (at the cost of a slower first run)!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cee1d6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<CompiledFunction of <function selu at 0x7f96e6c552d0>>\n",
      "46.7 µs ± 1.55 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "5.68 µs ± 73.3 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n",
      "2.46 ms ± 59.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "532 µs ± 18.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def selu(x, alpha=1.67, lmbda=1.05):\n",
    "    return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
    "\n",
    "selu_jit = jit(selu)\n",
    "print(selu_jit)\n",
    "\n",
    "x = random.normal(key, (100,))\n",
    "\n",
    "%timeit selu(x).block_until_ready()\n",
    "\n",
    "# note that selu_jit is compiled the first time it is called, and cached thereafter\n",
    "%timeit selu_jit(x).block_until_ready()\n",
    "\n",
    "\n",
    "# even faster than numpy for this one! (fused kernels!)\n",
    "def fn(x):\n",
    "    return x + x*x + x*x*x\n",
    "\n",
    "jax_fn = jit(fn)\n",
    "\n",
    "x = np.random.randn(1000, 1000).astype(dtype='float32')\n",
    "%timeit fn(x)\n",
    "\n",
    "x = jnp.array(x)\n",
    "%timeit jax_fn(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc1d5a3",
   "metadata": {},
   "source": [
    "### How does JIT work?\n",
    "\n",
    "To get a view of your Python code that is valid for many different argument values, JAX *traces* it on abstract values that represent sets of possible inputs. There are multiple different levels of abstraction, and different transformations use different abstraction levels.\n",
    "\n",
    "By default, jit traces your code on the `ShapedArray` abstraction level, where each abstract value represents the set of all array values with a fixed shape and dtype.\n",
    "\n",
    "This recorded sequence of computations can then be efficiently applied within XLA to new inputs *with the same shape and dtype*, without having to re-trace the Python code.\n",
    "\n",
    "Note other abstraction levels can be used - see guide.\n",
    "\n",
    "There are also more advanced control flow primitives in jax you can use to help avoid re-compilation, like `lax.cond`, `lax.scan`, `lax.while_loop` and `lax.fori_loop` - see guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f9c7a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running f():\n",
      "  x = Traced<ShapedArray(float32[3,4])>with<DynamicJaxprTrace(level=0/1)>\n",
      "  y = Traced<ShapedArray(float32[4])>with<DynamicJaxprTrace(level=0/1)>\n",
      "  result = Traced<ShapedArray(float32[3])>with<DynamicJaxprTrace(level=0/1)>\n",
      "Running f():\n",
      "  x = Traced<ShapedArray(float32[5,6])>with<DynamicJaxprTrace(level=0/1)>\n",
      "  y = Traced<ShapedArray(float32[6])>with<DynamicJaxprTrace(level=0/1)>\n",
      "  result = Traced<ShapedArray(float32[5])>with<DynamicJaxprTrace(level=0/1)>\n",
      "{ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; a\u001b[35m:f32[3,4]\u001b[39m b\u001b[35m:f32[4]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
      "    \u001b[39m\u001b[22m\u001b[22mc\u001b[35m:f32[3,4]\u001b[39m = add a 1.0\n",
      "    d\u001b[35m:f32[4]\u001b[39m = add b 1.0\n",
      "    e\u001b[35m:f32[3]\u001b[39m = dot_general[\n",
      "      dimension_numbers=(((1,), (0,)), ((), ()))\n",
      "      precision=None\n",
      "      preferred_element_type=None\n",
      "    ] c d\n",
      "  \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(e,) }\n",
      "{ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; a\u001b[35m:f32[3,4]\u001b[39m b\u001b[35m:f32[4]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
      "    \u001b[39m\u001b[22m\u001b[22mc\u001b[35m:f32[3]\u001b[39m = xla_call[\n",
      "      call_jaxpr={ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; d\u001b[35m:f32[3,4]\u001b[39m e\u001b[35m:f32[4]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
      "          \u001b[39m\u001b[22m\u001b[22mf\u001b[35m:f32[3,4]\u001b[39m = add d 1.0\n",
      "          g\u001b[35m:f32[4]\u001b[39m = add e 1.0\n",
      "          h\u001b[35m:f32[3]\u001b[39m = dot_general[\n",
      "            dimension_numbers=(((1,), (0,)), ((), ()))\n",
      "            precision=None\n",
      "            preferred_element_type=None\n",
      "          ] f g\n",
      "        \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(h,) }\n",
      "      name=f\n",
      "    ] a b\n",
      "  \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(c,) }\n"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def f(x, y):\n",
    "    print(\"Running f():\")\n",
    "    print(f\"  x = {x}\")\n",
    "    print(f\"  y = {y}\")\n",
    "    result = jnp.dot(x + 1, y + 1)\n",
    "    print(f\"  result = {result}\")\n",
    "    return result\n",
    "\n",
    "x = np.random.randn(3, 4)\n",
    "y = np.random.randn(4)\n",
    "f(x, y)\n",
    "\n",
    "# When we call the compiled function again on matching inputs, no re-compilation is  required \n",
    "# and nothing is printed because the result is computed in compiled XLA rather than in Python\n",
    "x2 = np.random.randn(3, 4)\n",
    "y2 = np.random.randn(4)\n",
    "f(x2, y2)\n",
    "\n",
    "# but if shapes (or types) change, we must re-compile\n",
    "x3 = np.random.randn(5, 6)\n",
    "y3 = np.random.randn(6)\n",
    "f(x3, y3)\n",
    "\n",
    "# but original compilation is still cached!!\n",
    "f(x, y)\n",
    "\n",
    "# The extracted sequence of operations is encoded in a JAX expression, or jaxpr for short. To view it:\n",
    "def f(x, y):\n",
    "    return jnp.dot(x + 1, y + 1)\n",
    "print(make_jaxpr(f)(x, y))\n",
    "print(make_jaxpr(jit(f))(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14057b9a",
   "metadata": {},
   "source": [
    "### JIT limitations\n",
    "\n",
    "Importantly, JIT compiling imposes restrictions on JAX code.\n",
    "\n",
    "In particular (for `ShapedArray` abstraction), it requires *array shapes* to be *static & known* at compile time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "727e61da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array boolean indices must be concrete; got ShapedArray(bool[10])\n",
      "\n",
      "See https://jax.readthedocs.io/en/latest/errors.html#jax.errors.NonConcreteBooleanIndexError\n"
     ]
    }
   ],
   "source": [
    "def get_negatives(x):\n",
    "    return x[x < 0]\n",
    "try:\n",
    "    jit(get_negatives)(jnp.arange(10))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac38c73",
   "metadata": {},
   "source": [
    "This crashes because the function generates an array whose shape is not known at compile time: the size of the output depends on the values of the input array, and so it is not compatible with JIT.\n",
    "\n",
    "Likewise, *control flow* statements in the function cannot depend on *traced values*, as JIT compilation is done without information on the content (values) of the array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6aaed986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(bool[], weak_type=True)>with<DynamicJaxprTrace(level=0/1)>\n",
      "The problem arose with the `bool` function. \n",
      "The error occurred while tracing the function f at /var/folders/6p/mdzn2wk57nq35tz4bgfd63300000gp/T/ipykernel_67556/2320181375.py:1 for jit. This concrete value was not available in Python because it depends on the value of the argument 'x'.\n",
      "\n",
      "See https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    if x > 0:\n",
    "        return x\n",
    "    else:\n",
    "        return 2 * x\n",
    "f_jit = jit(f)\n",
    "try:\n",
    "    f_jit(10)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839e55cd",
   "metadata": {},
   "source": [
    "Finally, jitted functions must be *pure*. (see above!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d62fc8",
   "metadata": {},
   "source": [
    "### Static variables/ operations\n",
    "\n",
    "If there are variables that you would not like to be traced, they can be marked as static for the purposes of JIT compilation.\n",
    "\n",
    "Note that calling a JIT-compiled function with a different static argument results in re-compilation, so the function still works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da0d1ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling!\n",
      "-1\n",
      "compiling!\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "@partial(jit, static_argnums=(1,))\n",
    "def f(x, neg):\n",
    "    print(\"compiling!\")\n",
    "    return -x if neg else x\n",
    "\n",
    "print(f(1, True))\n",
    "print(f(1, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3831904",
   "metadata": {},
   "source": [
    "Note `static_argnums` relies on the hash of the object to determine whether it has changed between calls.\n",
    "Lists and arrays aren't hashable (but tuples and ints are!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43f6445",
   "metadata": {},
   "source": [
    "Operations can also be made static.\n",
    "\n",
    "A useful pattern is to use `numpy` for operations that you want to be static, and `jax.numpy` for operations that you want to be traced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b98acaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes must be 1D sequences of concrete values of integer type, got [Traced<ShapedArray(int32[])>with<DynamicJaxprTrace(level=0/1)>].\n",
      "If using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([1., 1., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = jnp.ones((2, 3))\n",
    "\n",
    "@jit\n",
    "def f(x):\n",
    "    return x.reshape(jnp.array(x.shape).prod())\n",
    "# although x is traced, x.shape is a static value. However, when we use jnp.array and jnp.prod,\n",
    "# it becomes a traced value, at which point it cannot be used in a function like reshape() that \n",
    "# requires a static input (remember: jit requires array shapes to be static and known)\n",
    "\n",
    "try:\n",
    "    f(x)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "@jit\n",
    "def f(x):\n",
    "    return x.reshape((np.prod(x.shape),))# use numpy instead to make sure a static value is passed\n",
    "f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f223a680",
   "metadata": {},
   "source": [
    "### When to use JIT?\n",
    "\n",
    "`jax.jit` introduces some overhead itself. Therefore, it usually only saves time if the compiled function is complex and you will run it numerous times. Fortunately, this is common in machine learning, where we tend to compile a large, complicated model, then run it for millions of iterations.\n",
    "\n",
    "Generally, you want to jit the largest possible chunk of your computation; ideally, the *entire update step*. This gives the compiler maximum freedom to optimise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3c99e7",
   "metadata": {},
   "source": [
    "## Using `grad()`\n",
    "\n",
    "Taking derivatives is as easy as calling `grad()`. `grad()` and `jit()` compose and can be mixed arbitrarily.\n",
    "\n",
    "`grad()` works for scalar-valued function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8890288a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function grad in module jax._src.api:\n",
      "\n",
      "grad(fun: 'Callable', argnums: 'Union[int, Sequence[int]]' = 0, has_aux: 'bool' = False, holomorphic: 'bool' = False, allow_int: 'bool' = False, reduce_axes: 'Sequence[AxisName]' = ()) -> 'Callable'\n",
      "    Creates a function that evaluates the gradient of ``fun``.\n",
      "    \n",
      "    Args:\n",
      "      fun: Function to be differentiated. Its arguments at positions specified by\n",
      "        ``argnums`` should be arrays, scalars, or standard Python containers.\n",
      "        Argument arrays in the positions specified by ``argnums`` must be of\n",
      "        inexact (i.e., floating-point or complex) type. It\n",
      "        should return a scalar (which includes arrays with shape ``()`` but not\n",
      "        arrays with shape ``(1,)`` etc.)\n",
      "      argnums: Optional, integer or sequence of integers. Specifies which\n",
      "        positional argument(s) to differentiate with respect to (default 0).\n",
      "      has_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the\n",
      "        first element is considered the output of the mathematical function to be\n",
      "        differentiated and the second element is auxiliary data. Default False.\n",
      "      holomorphic: Optional, bool. Indicates whether ``fun`` is promised to be\n",
      "        holomorphic. If True, inputs and outputs must be complex. Default False.\n",
      "      allow_int: Optional, bool. Whether to allow differentiating with\n",
      "        respect to integer valued inputs. The gradient of an integer input will\n",
      "        have a trivial vector-space dtype (float0). Default False.\n",
      "      reduce_axes: Optional, tuple of axis names. If an axis is listed here, and\n",
      "        ``fun`` implicitly broadcasts a value over that axis, the backward pass\n",
      "        will perform a ``psum`` of the corresponding gradient. Otherwise, the\n",
      "        gradient will be per-example over named axes. For example, if ``'batch'``\n",
      "        is a named batch axis, ``grad(f, reduce_axes=('batch',))`` will create a\n",
      "        function that computes the total gradient while ``grad(f)`` will create\n",
      "        one that computes the per-example gradient.\n",
      "    \n",
      "    Returns:\n",
      "      A function with the same arguments as ``fun``, that evaluates the gradient\n",
      "      of ``fun``. If ``argnums`` is an integer then the gradient has the same\n",
      "      shape and type as the positional argument indicated by that integer. If\n",
      "      argnums is a tuple of integers, the gradient is a tuple of values with the\n",
      "      same shapes and types as the corresponding arguments. If ``has_aux`` is True\n",
      "      then a pair of (gradient, auxiliary_data) is returned.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    >>> import jax\n",
      "    >>>\n",
      "    >>> grad_tanh = jax.grad(jax.numpy.tanh)\n",
      "    >>> print(grad_tanh(0.2))\n",
      "    0.961043\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45a0b48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'function'> <class 'function'> <class 'function'>\n",
      "<function tanh at 0x7f96e86bc790> <function tanh at 0x7f96e86bca60> <function tanh at 0x7f96e86bcc10>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tanh(x):\n",
    "    return (1.0 - jnp.exp(-x))  / (1.0 + jnp.exp(-x))\n",
    "\n",
    "# get derivative functions\n",
    "tanh_d = grad(tanh)# this is a *function* for dy/dx, given x as input\n",
    "tanh_dd = grad(tanh_d)# similarly this is a *function* for d2y/dx2, given x as input\n",
    "\n",
    "print(type(tanh), type(tanh_d), type(tanh_dd))\n",
    "print(tanh, tanh_d, tanh_dd)\n",
    "\n",
    "x = jnp.linspace(-5, 5, 50)\n",
    "y = vmap(tanh)(x)\n",
    "dy = vmap(tanh_d)(x)# vmap explained below (for element-wise differentiation)!\n",
    "ddy = vmap(tanh_dd)(x)\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(x, y)\n",
    "plt.plot(x, dy)\n",
    "plt.plot(x, ddy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83541eca",
   "metadata": {},
   "source": [
    "### Advanced autodiff\n",
    "\n",
    "For more advanced autodiff, you can use `jax.vjp()` for reverse-mode vector-Jacobian products and `jax.jvp()` for forward-mode Jacobian-vector products. The two can be composed arbitrarily with one another, and with other JAX transformations. Here’s one way to compose them to make a function that efficiently computes full Hessian matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72d9da38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(fun):\n",
    "    return jit(jacfwd(jacrev(fun)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9084841c",
   "metadata": {},
   "source": [
    "## Using `vmap()`\n",
    "\n",
    "Stands for vectorising map. It has the familiar semantics of mapping a function along array axes, but instead of keeping the loop on the outside, it *pushes the loop down into a function’s primitive operations* for better performance. When composed with `jit()`, it can be just as fast as adding the batch dimensions by hand.\n",
    "\n",
    "It does this by tracing the function similarly to `jax.jit`, and automatically adding batch axes at the beginning of each input.\n",
    "\n",
    "If the batch dimension is not the first, you may use the `in_axes` and `out_axes` arguments to specify the location of the batch dimension in inputs and outputs. These may be an integer if the batch axis is the same for all inputs and outputs, or lists, otherwise.\n",
    "\n",
    "Note you can only `vmap` over array inputs! (and not functions etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44a5a74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naively batched\n",
      "21.4 µs ± 78.2 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "Manually batched\n",
      "16.1 µs ± 351 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n",
      "Auto-vectorized with vmap\n",
      "24.3 µs ± 725 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "mat = random.normal(key, (150, 100))\n",
    "batched_x = random.normal(key, (10, 100))\n",
    "\n",
    "def apply_matrix(v):\n",
    "    return jnp.dot(mat, v)\n",
    "\n",
    "@jit\n",
    "def naively_batched_apply_matrix(v_batched):\n",
    "    return jnp.stack([apply_matrix(v) for v in v_batched])# naive for loop\n",
    "\n",
    "@jit\n",
    "def manually_batched_apply_matrix(v_batched):\n",
    "    return jnp.dot(v_batched, mat.T)# np just handles it as usual\n",
    "\n",
    "@jit\n",
    "def vmap_batched_apply_matrix(v_batched):\n",
    "    return vmap(apply_matrix)(v_batched)# vmap the function\n",
    "\n",
    "print('Naively batched')\n",
    "%timeit naively_batched_apply_matrix(batched_x).block_until_ready()# probably jit figures it out for easy case\n",
    "\n",
    "print('Manually batched')\n",
    "%timeit manually_batched_apply_matrix(batched_x).block_until_ready()\n",
    "\n",
    "print('Auto-vectorized with vmap')\n",
    "%timeit vmap_batched_apply_matrix(batched_x).block_until_ready()\n",
    "\n",
    "# Note, the same technique can be applied to more complicated functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e654422e",
   "metadata": {},
   "source": [
    "### Pytrees\n",
    "\n",
    "Many JAX functions, like `jax.lax.scan()` and function transformations, like `vmap()`, can operate over *pytrees* of arrays, rather than single arrays. \n",
    "\n",
    "A *pytree* is any tree-like structure built out of container-like Python objects (lists, tuples, dicts), such as \n",
    "\n",
    "`[1, \"a\", object()]` or\n",
    "`[1, {\"k1\": 2, \"k2\": (3, 4)}, 5]`\n",
    "\n",
    "Such trees are useful for e.g. Model parameters and other training inputs.\n",
    "\n",
    "JAX has some utility functions in `jax.tree_utils` to help check these objects too.\n",
    "\n",
    "Perhaps the most commonly used pytree function `is jax.tree_map`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25cf85e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 4, 6], [2, 4], [2, 4, 6, 8]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = [\n",
    "    [1, 2, 3],\n",
    "    [1, 2],\n",
    "    [1, 2, 3, 4]\n",
    "]\n",
    "\n",
    "jax.tree_map(lambda x: x*2, tree)\n",
    "# It works analogously to Python’s native map, but on entire pytrees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc4b36d",
   "metadata": {},
   "source": [
    "or indeed, jax.tree_map can be used to update all parameters in SGD\n",
    "\n",
    "`return jax.tree_map(lambda p, g: p - LEARNING_RATE * g, params, grads)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3a88fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 1, 2, 1, 2, 3, 4]\n",
      "PyTreeDef([[*, *, *], [*, *], [*, *, *, *]])\n"
     ]
    }
   ],
   "source": [
    "print(tree_util.tree_leaves(tree))\n",
    "print(tree_util.tree_structure(tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcc24ec",
   "metadata": {},
   "source": [
    "You can also define your own pytree too"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e15811",
   "metadata": {},
   "source": [
    "## Using `pmap()`\n",
    "\n",
    "Consider the example of vector-matrix multiplication. Let’s say we are performing this computation by sequentially computing the dot product of the vector with each row of the matrix. We would need to push these computations through our hardware one at a time.\n",
    "\n",
    "With JAX, we can easily distribute these computations across 4 TPUs by simply wrapping our operation in `pmap()`. This allows us to concurrently perform one dot product on each TPU, significantly increasing our computation speed (for large computations).\n",
    "\n",
    "The purpose of `pmap()` is to express single-program multiple-data (SPMD) programs. Applying `pmap()` to a function will compile the function with XLA (similarly to `jit()`), then execute it in parallel on XLA devices, such as multiple GPUs or multiple TPU cores. \n",
    "\n",
    "Semantically it is comparable to `vmap()` because both transformations map a function over array axes, but where vmap() vectorizes functions by pushing the mapped axis down into primitive operations, `pmap()` instead replicates the function and executes each replica on its own XLA device in parallel.\n",
    "\n",
    "The mapped axis size must be less than or equal to the number of local XLA devices available, as returned by `jax.local_device_count()` (unless devices is specified, see below). For nested `pmap()` calls, the product of the mapped axis sizes must be less than or equal to the number of XLA devices.\n",
    "\n",
    "> Note: `pmap()` compiles `fun`, so while it can be combined with jit(), it’s usually unnecessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0d9f930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CpuDevice(id=0)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6f03243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShardedDeviceArray([0], dtype=int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    y = x**2\n",
    "    return y\n",
    "\n",
    "pmap(f, in_axes=0)(jnp.arange(1))# need to parallelise across devices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179e877c",
   "metadata": {},
   "source": [
    "Note that the parallelized convolve returns a ShardedDeviceArray. That is because the elements of this array are sharded across all of the devices used in the parallelism. If we were to run another parallel computation, the elements would stay on their respective devices, without incurring cross-device communication costs.\n",
    "\n",
    "Note, however, that unlike vmap, only the leading axis (0) is supported by pmap at the time of writing this guide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219a70da",
   "metadata": {},
   "source": [
    "### Communication between devices\n",
    "\n",
    "However, sometimes we need to pass information between the devices. For example, perhaps we are interested in normalizing the output of each device so they sum to 1. For that, we can use special collective ops (such as the `jax.lax.p*` ops psum, pmean, pmax, …). In order to use the collective ops we must specify the name of the `pmap`-ed axis through `axis_name` argument, and then refer to it when calling the op. Here’s how to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "074d322e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShardedDeviceArray([1.], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    y = x**2\n",
    "    return y/jax.lax.psum(y, axis_name=\"p\")\n",
    "\n",
    "pmap(f, in_axes=0, axis_name=\"p\")(jnp.arange(2,3))# need to parallelise across devices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41912f32",
   "metadata": {},
   "source": [
    "The `axis_name` is just a string label that allows collective operations like `jax.lax.psum` to refer to the axis bound by `jax.pmap`. It can be named anything you want – in this case, `p`. This name is essentially invisible to anything but those functions, and those functions use it to know which axis to communicate across.\n",
    "\n",
    "`jax.vmap` also supports axis_name, which allows `jax.lax.p*` operations to be used in the vectorisation context in the same way they would be used in a `jax.pmap`.\n",
    "\n",
    "Check out here an example for parallel regression: https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html#example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324a2de8",
   "metadata": {},
   "source": [
    "## Simple training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc15cffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xs = np.random.normal(size=(100,))\n",
    "noise = np.random.normal(scale=0.1, size=(100,))\n",
    "ys = 3 * xs - 1 + noise\n",
    "\n",
    "def model(theta, x):\n",
    "    w, b = theta\n",
    "    return w * x + b\n",
    "\n",
    "def loss_fn(theta, x, y):\n",
    "    prediction = model(theta, x)\n",
    "    return jnp.mean((prediction-y)**2)\n",
    "\n",
    "@jit# jit is WAY faster\n",
    "def update(theta, x, y, lr=0.1):\n",
    "    return theta - lr * grad(loss_fn, argnums=0)(theta, x, y)\n",
    "\n",
    "theta = jnp.array([1., 1.])\n",
    "\n",
    "for _ in range(10000):\n",
    "    theta = update(theta, xs, ys)\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.scatter(xs, ys)\n",
    "plt.plot(xs, model(theta, xs), color=\"tab:green\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dad0f7",
   "metadata": {},
   "source": [
    "## Simple NN\n",
    "\n",
    "Notes:\n",
    "- We define forward model for a single image example, and use JAX’s `vmap` function to automatically handle mini-batches, with no performance penalty.\n",
    "- We use the whole of the JAX API: `grad` for derivatives, `jit` for speedups, `vmap` for auto-vectorization, `jnp` to specify all of our computation, data loaders from PyTorch, and run on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6abcb9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_layer_params(m, n, key, scale=1e-2):\n",
    "    w_key, b_key = random.split(key)\n",
    "    return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n",
    "\n",
    "def init_network_params(sizes, key):\n",
    "    keys = random.split(key, len(sizes))\n",
    "    return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
    "\n",
    "def relu(x):\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "def predict(params, image):\n",
    "    # PER EXAMPLE predictions\n",
    "    activations = image\n",
    "    for w, b in params[:-1]:\n",
    "        outputs = jnp.dot(w, activations) + b\n",
    "        activations = relu(outputs)\n",
    "    final_w, final_b = params[-1]\n",
    "    logits = jnp.dot(final_w, activations) + final_b\n",
    "    return logits - logsumexp(logits)\n",
    "\n",
    "batched_predict = vmap(predict, in_axes=(None, 0))\n",
    "\n",
    "def loss(params, images, targets):\n",
    "    preds = batched_predict(params, images)\n",
    "    return -jnp.mean(preds * targets)\n",
    "\n",
    "@jit# just jit the whole thing!\n",
    "def update(params, x, y):\n",
    "    grads = grad(loss)(params, x, y)\n",
    "    return [(w - step_size * dw, b - step_size * db) for (w, b), (dw, db) in zip(params, grads)]\n",
    "\n",
    "#for epoch in range(num_epochs):\n",
    "#    for x, y in dataloader:\n",
    "#    params = update(params, x, y)\n",
    "\n",
    "\n",
    "layer_sizes = [784, 512, 512, 10]\n",
    "step_size = 0.01\n",
    "num_epochs = 8\n",
    "batch_size = 128\n",
    "n_targets = 10\n",
    "params = init_network_params(layer_sizes, random.PRNGKey(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
